{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INF506 Methods of Data Analysis - Term Project\n## Data Science Project: Athens Housing Market Analysis\n\n**Group Members:**\n* Ali Hamza Pe√ßenek (220503003)\n* Hatice K√ºbra Alaca (230503075)\n* [Add other group members here]\n\n**Date:** January 2026\n\n### Project Overview\nIn this project, we analyze a real-world dataset of housing listings in Athens, Greece, scraped from *spitogatos.gr*. The goal is to demonstrate skills in data collection, cleaning, exploration, statistical analysis, and machine learning.","metadata":{}},{"cell_type":"code","source":"# General Data Analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nimport random\nimport os\nimport warnings\nfrom datetime import datetime\n\n# Web Scraping\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\n# Machine Learning & Statistics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Configuration\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nprint(\"Libraries imported successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Data Collection\nWe collected housing data using Python's `Selenium` library. The target website was *spitogatos.gr*. We focused on two main areas: **Athens Center** and **Athens North**.\n\n**Note:** The scraping code below is configured to run, but commented out by default to prevent accidental execution (as scraping 200+ pages takes significant time).","metadata":{}},{"cell_type":"code","source":"# Configuration for Areas\nAREAS = {\n    'athens-center': {\n        'url': 'https://www.spitogatos.gr/en/for_sale-homes/athens-center',\n        'pages': 100 \n    },\n    'athens-north': {\n        'url': 'https://www.spitogatos.gr/en/for_sale-homes/athens-north',\n        'pages': 100\n    }\n}\n\nAUTOSAVE_INTERVAL = 1000\n\ndef setup_driver():\n    \"\"\"Setup Chrome driver with anti-detection measures\"\"\"\n    chrome_options = Options()\n    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n    chrome_options.add_experimental_option('useAutomationExtension', False)\n    chrome_options.add_argument('--disable-dev-shm-usage')\n    chrome_options.add_argument('--no-sandbox')\n    chrome_options.add_argument('--window-size=1920,1080')\n    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n    \n    service = Service(ChromeDriverManager().install())\n    driver = webdriver.Chrome(service=service, options=chrome_options)\n    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n    return driver\n\ndef scrape_listing(article):\n    \"\"\"Extract data from a single listing element\"\"\"\n    try:\n        data = {}\n        # Property Type and Size\n        try:\n            title = article.find_element(By.CSS_SELECTOR, 'h3.tile__title').text\n            if ',' in title:\n                parts = title.split(',')\n                data['property_type'] = parts[0].strip()\n                data['size'] = parts[1].strip() if len(parts) > 1 else None\n            else:\n                data['property_type'] = title\n                data['size'] = None\n        except:\n            data['property_type'] = None\n            data['size'] = None\n        \n        # Location\n        try:\n            data['location'] = article.find_element(By.CSS_SELECTOR, 'h3.tile__location').text\n        except:\n            data['location'] = None\n        \n        # Description\n        try:\n            data['description'] = article.find_element(By.CSS_SELECTOR, 'p.tile__description').text\n        except:\n            data['description'] = None\n        \n        # Price\n        try:\n            data['price'] = article.find_element(By.CSS_SELECTOR, 'p.price__text').text\n        except:\n            data['price'] = None\n        \n        # Floor, Bedrooms, Bathrooms\n        data['floor'] = None\n        data['bedrooms'] = None\n        data['bathrooms'] = None\n        \n        try:\n            info_items = article.find_elements(By.CSS_SELECTOR, 'ul.tile__info li')\n            for item in info_items:\n                title_attr = item.get_attribute('title').lower() if item.get_attribute('title') else ''\n                text = item.text\n                if 'floor' in title_attr:\n                    data['floor'] = text\n                elif 'bedroom' in title_attr:\n                    data['bedrooms'] = ''.join(filter(str.isdigit, text))\n                elif 'bathroom' in title_attr:\n                    data['bathrooms'] = ''.join(filter(str.isdigit, text))\n        except:\n            pass\n        \n        # URL\n        try:\n            link = article.find_element(By.CSS_SELECTOR, 'a.tile__link')\n            data['url'] = link.get_attribute('href')\n        except:\n            data['url'] = None\n        \n        return data\n    except Exception as e:\n        return None\n\ndef scrape_page(driver, url):\n    \"\"\"Scrape a single page\"\"\"\n    try:\n        print(f\"  Loading: {url}\")\n        driver.get(url)\n        time.sleep(random.uniform(3, 6))\n        \n        total_height = driver.execute_script(\"return document.body.scrollHeight\")\n        for i in range(1, 4):\n            driver.execute_script(f\"window.scrollTo(0, {total_height * i / 4});\")\n            time.sleep(random.uniform(0.5, 1.5))\n        \n        try:\n            WebDriverWait(driver, 10).until(\n                EC.presence_of_element_located((By.CSS_SELECTOR, \"article.ordered-element\"))\n            )\n        except:\n            return []\n        \n        articles = driver.find_elements(By.CSS_SELECTOR, \"article.ordered-element\")\n        listings = []\n        for article in articles:\n            listing_data = scrape_listing(article)\n            if listing_data and listing_data.get('url'):\n                listings.append(listing_data)\n        return listings\n    except Exception as e:\n        return []\n\ndef scrape_area(driver, area_name, base_url, pages_for_area):\n    print(f\"üìç SCRAPING AREA: {area_name.upper()}\")\n    all_listings = []\n    \n    for page_num in range(1, pages_for_area + 1):\n        if page_num == 1:\n            url = base_url\n        else:\n            url = f\"{base_url}/page_{page_num}\"\n        \n        listings = scrape_page(driver, url)\n        if not listings:\n            break\n        \n        all_listings.extend(listings)\n        print(f\"  Total from {area_name}: {len(all_listings)} listings\")\n        time.sleep(random.uniform(2, 5))\n    \n    return all_listings\n\n# ==========================================================\n# UNCOMMENT THE LINES BELOW TO RUN THE SCRAPER\n# ==========================================================\n# if __name__ == \"__main__\":\n#     driver = setup_driver()\n#     full_data = []\n#     for area, info in AREAS.items():\n#         area_data = scrape_area(driver, area, info['url'], info['pages'])\n#         full_data.extend(area_data)\n#         # Save individual files if needed\n#         pd.DataFrame(area_data).to_csv(f\"{area}.csv\", index=False)\n#     driver.quit()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming the files 'athens_center.csv' and 'athens_north.csv' are in the project folder\n# If you scraped them just now, they should be there.\n\ntry:\n    file1 = \"athens_center.csv\"\n    file2 = \"athens_north.csv\"\n\n    # Check if files exist, otherwise mock data for demonstration or raise error\n    if os.path.exists(file1) and os.path.exists(file2):\n        df1 = pd.read_csv(file1)\n        df2 = pd.read_csv(file2)\n        \n        # Combine them (row-wise)\n        df_raw = pd.concat([df1, df2], ignore_index=True)\n        print(f\"Files combined successfully. Total shape: {df_raw.shape}\")\n        \n        # Save the combined raw file for the cleaning step\n        df_raw.to_csv('athens_complete_house_data_raw.csv', index=False)\n    else:\n        print(\"Warning: CSV files not found. Please ensure 'athens_center.csv' and 'athens_north.csv' are in the directory.\")\n        # Fallback to loading the provided raw file if it exists\n        if os.path.exists('athens_complete_house_data_raw.csv'):\n            df_raw = pd.read_csv('athens_complete_house_data_raw.csv')\n            print(\"Loaded existing 'athens_complete_house_data_raw.csv'\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cell 6: [Markdown] - Part 2: Data Cleaning & EDA\n\nIn this section, we perform comprehensive data cleaning:\n\n\n    ** Handling duplicates. **\n\n    **Standardizing formats (Price, Size, Floor).**\n\n    **Imputing missing values.**\n\n    **Removing outliers.**\n\n    **Feature Engineering.**","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Cleaning & EDA\n\nIn this section, we perform comprehensive data cleaning:\n\n**Handling duplicates..**\n\n**Standardizing formats (Price, Size, Floor).**\n\n**Imputing missing values.**\n\n**Removing outliers.**\n\n**Feature Engineering.**","metadata":{}},{"cell_type":"code","source":"### **Cell 7: [Code] - Data Cleaning Implementation**\n*This cell processes the raw data and outputs a clean dataset.*\n\n```python\nprint(\"=\"*80)\nprint(\"STARTING DATA CLEANING\")\nprint(\"=\"*80)\n\n# Load the raw scraped data\ntry:\n    df_raw = pd.read_csv('athens_complete_house_data_raw.csv')\nexcept FileNotFoundError:\n    print(\"Error: 'athens_complete_house_data_raw.csv' not found.\")\n    # Stop execution if file missing\n    raise\n\n# Create working copy\ndf = df_raw.copy()\n\n# 1. REMOVE DUPLICATES\n# Remove duplicates based on URL\ndf = df.drop_duplicates(subset=['url'], keep='first')\n# Remove exact duplicates\ndf = df.drop_duplicates(keep='first')\n\n# 2. CLEAN PROPERTY TYPE\ndf['property_type'] = df['property_type'].str.strip()\n\n# 3. CLEAN SIZE\ndef clean_size(size_str):\n    if pd.isna(size_str):\n        return np.nan\n    match = re.search(r'(\\d+(?:\\.\\d+)?)', str(size_str))\n    if match:\n        return float(match.group(1))\n    return np.nan\n\ndf['size_sqm'] = df['size'].apply(clean_size)\n\n# 4. CLEAN PRICE\ndef clean_price(price_str):\n    if pd.isna(price_str):\n        return np.nan\n    price_clean = str(price_str).replace('‚Ç¨', '').replace(',', '').replace(' ', '')\n    match = re.search(r'(\\d+(?:\\.\\d+)?)', price_clean)\n    if match:\n        return float(match.group(1))\n    return np.nan\n\ndf['price_eur'] = df['price'].apply(clean_price)\n\n# 5. CLEAN FLOOR\ndef clean_floor(floor_str):\n    if pd.isna(floor_str):\n        return np.nan\n    floor_str = str(floor_str).strip().lower()\n    if floor_str in ['g', 'ground', 'ground floor']: return 0\n    if floor_str in ['b', 'basement']: return -1\n    if floor_str in ['semi-basement', 'semi basement']: return -0.5\n    if floor_str in ['mezzanine']: return 0.5\n    \n    floor_clean = re.sub(r'\\\\n[a-z]{2}', '', floor_str)\n    floor_clean = re.sub(r'(st|nd|rd|th)', '', floor_clean)\n    match = re.search(r'(-?\\d+(?:\\.\\d+)?)', floor_clean)\n    if match:\n        return float(match.group(1))\n    return np.nan\n\ndf['floor_number'] = df['floor'].apply(clean_floor)\n\n# 6. CLEAN LOCATION\ndf['location_clean'] = df['location'].str.strip()\ndf['neighborhood'] = df['location_clean'].str.extract(r'^([^(]+)')[0].str.strip()\ndf['district'] = df['location_clean'].str.extract(r'\\(([^)]+)\\)')[0]\n\n# 7. FEATURE ENGINEERING (Price/Sqm)\ndf['price_per_sqm'] = df['price_eur'] / df['size_sqm']\n\n# 8. ADDITIONAL FEATURES\ndf['bath_bed_ratio'] = df['bathrooms'] / df['bedrooms']\ndf['bath_bed_ratio'] = df['bath_bed_ratio'].replace([np.inf, -np.inf], np.nan)\n\n# Categorizations\ndef categorize_size(size):\n    if pd.isna(size): return 'Unknown'\n    elif size < 50: return 'Small'\n    elif size < 100: return 'Medium'\n    elif size < 200: return 'Large'\n    else: return 'Extra Large'\n\ndef categorize_price(price):\n    if pd.isna(price): return 'Unknown'\n    elif price < 100000: return 'Budget'\n    elif price < 250000: return 'Moderate'\n    elif price < 500000: return 'Premium'\n    else: return 'Luxury'\n\ndef categorize_floor(floor):\n    if pd.isna(floor): return 'Unknown'\n    elif floor < 0: return 'Basement'\n    elif floor == 0: return 'Ground Floor'\n    elif floor <= 2: return 'Low Floor'\n    elif floor <= 5: return 'Mid Floor'\n    else: return 'High Floor'\n\ndf['size_category'] = df['size_sqm'].apply(categorize_size)\ndf['price_category'] = df['price_eur'].apply(categorize_price)\ndf['floor_category'] = df['floor_number'].apply(categorize_floor)\n\n# 9. IMPUTATION\n# Fill missing values with median\ndf['bedrooms'].fillna(df['bedrooms'].median(), inplace=True)\ndf['bathrooms'].fillna(df['bathrooms'].median(), inplace=True)\ndf['floor_number'].fillna(df['floor_number'].median(), inplace=True)\n\n# 10. OUTLIER REMOVAL (Domain Knowledge)\ninitial_len = len(df)\ndf = df[(df['size_sqm'].isna()) | ((df['size_sqm'] >= 15) & (df['size_sqm'] <= 2000))]\ndf = df[(df['price_eur'].isna()) | ((df['price_eur'] >= 10000) & (df['price_eur'] <= 10000000))]\ndf = df[(df['bedrooms'].isna()) | (df['bedrooms'] <= 15)]\ndf = df[(df['bathrooms'].isna()) | (df['bathrooms'] <= 15)]\ndf = df[(df['floor_number'].isna()) | ((df['floor_number'] >= -2) & (df['floor_number'] <= 50))]\ndf = df[(df['price_per_sqm'].isna()) | ((df['price_per_sqm'] >= 500) & (df['price_per_sqm'] <= 15000))]\n\n# Final Select\nfinal_columns = [\n    'property_type', 'size_sqm', 'price_eur', 'bedrooms', 'bathrooms', \n    'floor_number', 'neighborhood', 'district', 'location_clean', \n    'description', 'url', 'price_per_sqm', 'bath_bed_ratio', \n    'size_category', 'price_category', 'floor_category'\n]\ndf_clean = df[final_columns].copy()\n\nprint(f\"Cleaning Complete.\")\nprint(f\"Original Rows: {len(df_raw)}\")\nprint(f\"Clean Rows: {len(df_clean)}\")\nprint(\"-\" * 30)\nprint(df_clean.describe())\n\n# Save for next steps\ndf_clean.to_csv('athens_housing_data_clean.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation Matrix\nnumerical_features = ['size_sqm', 'price_eur', 'bedrooms', 'bathrooms', 'floor_number', 'price_per_sqm']\ncorrelation_matrix = df_clean[numerical_features].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix')\nplt.show()\n\n# Price vs Size Scatter\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_clean, x='size_sqm', y='price_eur', hue='property_type', alpha=0.6)\nplt.title('Price vs Size')\nplt.xlabel('Size (sqm)')\nplt.ylabel('Price (EUR)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Wahrscheinlichkeitsrechnung (Probability Analysis)\n\n**Ereignisse (Events):**\n* **Ereignis A:** Die Miete (Preis) des Hauses liegt √ºber dem Durchschnitt --> \"Teures Haus\"\n* **Ereignis B:** Die Quadratmeterzahl des Hauses liegt √ºber dem Durchschnitt --> \"Gro√ües Haus\"","metadata":{}},{"cell_type":"code","source":"# Wir verwenden den gereinigten Datensatz\ndf = pd.read_csv('athens_housing_data_clean.csv')\n\n# Durchschnittswerte berechnen\nprice_mean = df['price_eur'].mean()\nprint(f\"Durchschnittlicher Preis: ‚Ç¨{price_mean:,.2f}\")\n\nsize_mean = df['size_sqm'].mean()\nprint(f\"Durchschnittliche Gr√∂√üe: {size_mean:.2f} m¬≤\")\n\n# Ereignisse definieren\nereignis_A = df[df['price_eur'] > price_mean]\nereignis_B = df[df['size_sqm'] > size_mean]\nintersection_AB = df[(df['price_eur'] > price_mean) & (df['size_sqm'] > size_mean)]\ntotal_houses = len(df)\n\n# Wahrscheinlichkeiten berechnen\nP_A = len(ereignis_A) / total_houses\nprint(f\"P(A) (Teuer): {P_A:.4f}\")\n\nP_B = len(ereignis_B) / total_houses\nprint(f\"P(B) (Gro√ü): {P_B:.4f}\")\n\nP_AB = len(intersection_AB) / total_houses\nprint(f\"P(A ‚à© B): {P_AB:.4f}\")\n\n# Bedingte Wahrscheinlichkeiten\n# P(A | B) = P(A ‚à© B) / P(B)\nif P_B > 0:\n    p_A_given_B = P_AB / P_B\n    print(f\"P(A | B) (Teuer wenn Gro√ü): {p_A_given_B:.4f}\")\nelse:\n    print(\"P(B) ist 0\")\n\n# P(B | A) = P(A ‚à© B) / P(A)\nif P_A > 0:\n    p_B_given_A = P_AB / P_A\n    print(f\"P(B | A) (Gro√ü wenn Teuer): {p_B_given_A:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Maschinelles Lernen\nF√ºr die Preisprognose werden folgende Modelle trainiert und verglichen:\n1.  Linear Regression\n2.  Random Forest Regressor\n3.  K-Nearest Neighbors (KNN)\n4.  Neuronale Netze (Neural Networks)\n\n**Vorverarbeitung (Preprocessing):**\n* Entfernen nicht relevanter Spalten (URL, Description).\n* One-Hot Encoding f√ºr kategorische Variablen (`district`, `property_type`).\n* Skalierung der Daten mit `StandardScaler`.","metadata":{}},{"cell_type":"code","source":"# Daten neu laden um sicherzugehen\ndf = pd.read_csv('athens_housing_data_clean.csv')\n\n# Nicht erforderliche Spalten l√∂schen\n# price_per_sqm wird gel√∂scht, da es direkt vom Ziel (price) abh√§ngt (Data Leakage vermeiden)\ncols_to_drop = ['description', 'url', 'location_clean', 'size_category', \n                'price_category', 'floor_category', 'bath_bed_ratio', \n                'floor_number', 'neighborhood', 'price_per_sqm']\n\n# √úberpr√ºfen ob Spalten existieren bevor wir l√∂schen\ncols_to_drop = [c for c in cols_to_drop if c in df.columns]\ndf.drop(columns=cols_to_drop, inplace=True)\n\n# One-Hot Encoding\ndf = pd.get_dummies(df, columns=['district', 'property_type'], drop_first=True)\n\n# Fehlende Werte entfernen (falls noch vorhanden)\ndf = df.dropna()\n\n# Features (X) und Target (y)\nX = df.drop(columns=['price_eur'])\ny = df['price_eur']\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training Shape: {X_train.shape}\")\nprint(f\"Test Shape: {X_test.shape}\")\n\n# Skalierung\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Linear Regression\nprint(\"--- Linear Regression ---\")\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\ny_pred_lr = lr_model.predict(X_test)\nprint(f\"R2: {r2_score(y_test, y_pred_lr):.4f}\")\nprint(f\"RMSE: ‚Ç¨{np.sqrt(mean_squared_error(y_test, y_pred_lr)):,.2f}\")\n\n# 2. Random Forest\nprint(\"\\n--- Random Forest ---\")\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\nprint(f\"R2: {r2_score(y_test, y_pred_rf):.4f}\")\nprint(f\"RMSE: ‚Ç¨{np.sqrt(mean_squared_error(y_test, y_pred_rf)):,.2f}\")\n\n# 3. K-Nearest Neighbors\nprint(\"\\n--- KNN ---\")\nknn_model = KNeighborsRegressor()\nknn_model.fit(X_train, y_train)\ny_pred_knn = knn_model.predict(X_test)\nprint(f\"R2: {r2_score(y_test, y_pred_knn):.4f}\")\nprint(f\"RMSE: ‚Ç¨{np.sqrt(mean_squared_error(y_test, y_pred_knn)):,.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Grid Search (Random Forest) ---\")\n\n# Parameter Grid definieren\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5]\n}\n\nrf2 = RandomForestRegressor(random_state=42)\ngrid_search_rf = GridSearchCV(estimator=rf2, param_grid=param_grid, \n                              cv=3, n_jobs=-1, verbose=1, scoring='r2')\ngrid_search_rf.fit(X_train, y_train)\n\nprint(f\"Beste Parameter: {grid_search_rf.best_params_}\")\n\nbest_rf = grid_search_rf.best_estimator_\ny_pred_best = best_rf.predict(X_test)\n\nprint(f\"Optimiertes R2: {r2_score(y_test, y_pred_best):.4f}\")\nprint(f\"Optimiertes RMSE: ‚Ç¨{np.sqrt(mean_squared_error(y_test, y_pred_best)):,.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Neuronale Netze (Deep Learning)\nWir verwenden ein Feed-Forward Neural Network mit mehreren Dense-Layern.","metadata":{}},{"cell_type":"code","source":"print(\"--- Neural Network Training ---\")\n\n# Modell Aufbau\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=X_train.shape[1]))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='linear')) # Output layer for regression\n\n# Kompilieren\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Early Stopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n\n# Training\nhistory = model.fit(x=X_train, \n                    y=y_train, \n                    validation_data=(X_test, y_test), \n                    batch_size=16,   \n                    epochs=100, # Reduced epochs for faster demonstration\n                    callbacks=[early_stop],\n                    verbose=0) # Set verbose=1 to see training progress\n\n# Plot Loss\nloss_df = pd.DataFrame(model.history.history)\nloss_df[['loss', 'val_loss']].plot(figsize=(10,6))\nplt.title(\"Training Loss vs Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\")\nplt.show()\n\n# Vorhersage & Evaluation\ny_pred_nn = model.predict(X_test)\nr2_nn = r2_score(y_test, y_pred_nn)\nrmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n\nprint(f\"Neural Network R2: {r2_nn:.4f}\")\nprint(f\"Neural Network RMSE: ‚Ç¨{rmse_nn:,.0f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion & Results\nIn this project, we successfully scraped housing data from Athens, cleaned it, and applied various machine learning models to predict housing prices.\n* **Data Quality:** After cleaning, we retained ~96% of the original data.\n* **Correlations:** Size (sqm) and Price had the strongest correlation (~0.74).\n* **Model Performance:** The **Random Forest** and **Neural Network** models performed best, achieving R¬≤ scores between 0.68 - 0.73.","metadata":{}}]}